{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big G Express - Data Exploration\n",
    "\n",
    "## Team: Elden Ring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://eldenring.wiki.fextralife.com/file/Elden-Ring/mirel_pastor_of_vow.jpg\" alt=\"PRAISE DOG\" style=\"width:806px;height:600px;\"/>\n",
    "\n",
    "#### PRAISE THE DOG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults = pd.read_pickle('../data/faults_filtered.pkl')\n",
    "#y_derate = pd.read_pickle('../data/target_derate.pkl') # this one is the starting/base model, 6 hr\n",
    "y_derate = pd.read_pickle('../data/target_derate3h.pkl')\n",
    "#y_derate = pd.read_pickle('../data/target_derate12h.pkl')\n",
    "#y_derate = pd.read_pickle('../data/target_derate24h.pkl')\n",
    "# y_derate = pd.read_pickle('../data/target_derate1wk.pkl')\n",
    "#y_derate = pd.read_pickle('../data/target_derate6h_noderaterow.pkl')\n",
    "#y_75derate = pd.read_pickle('../data/target_75derate.pkl')\n",
    "diagnostics_imputed = pd.read_pickle('../data/diagnostics_imputed.pkl')\n",
    "#diagnostics_imputed = pd.read_pickle('../data/diagnostics_imputed_median.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is mostly NaNs, just 250 values or so\n",
    "diagnostics_imputed = diagnostics_imputed.drop(columns='ServiceDistance')\n",
    "\n",
    "# and this drops columns that are not useful for predictions\n",
    "faults = faults.drop(columns=['ESS_Id', 'active', 'eventDescription','ecuSoftwareVersion', 'ecuSerialNumber', \n",
    "    'ecuModel', 'ecuMake', 'ecuSource', 'MCTNumber', 'Latitude', 'Longitude', 'LocationTimeStamp'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember there are parts of columns (where a particular truck had no values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was just a simple fill with mean..\n",
    "diagnostics_imputed['AcceleratorPedal'] = diagnostics_imputed['AcceleratorPedal'].fillna(value=diagnostics_imputed['AcceleratorPedal'].mean())\n",
    "diagnostics_imputed['CruiseControlSetSpeed'] = diagnostics_imputed['CruiseControlSetSpeed'].fillna(value=diagnostics_imputed['CruiseControlSetSpeed'].mean())\n",
    "diagnostics_imputed['EngineTimeLtd'] = diagnostics_imputed['EngineTimeLtd'].fillna(value=diagnostics_imputed['EngineTimeLtd'].mean())\n",
    "diagnostics_imputed['FuelLevel'] = diagnostics_imputed['FuelLevel'].fillna(value=diagnostics_imputed['FuelLevel'].mean())\n",
    "diagnostics_imputed['FuelTemperature'] = diagnostics_imputed['FuelTemperature'].fillna(value=diagnostics_imputed['FuelTemperature'].mean())\n",
    "diagnostics_imputed['SwitchedBatteryVoltage'] = diagnostics_imputed['SwitchedBatteryVoltage'].fillna(value=diagnostics_imputed['SwitchedBatteryVoltage'].mean())\n",
    "diagnostics_imputed['Throttle'] = diagnostics_imputed['Throttle'].fillna(value=diagnostics_imputed['Throttle'].mean())\n",
    "\n",
    "#same but when using median - slightly worse than the mean\n",
    "# diagnostics_imputed['AcceleratorPedal'] = diagnostics_imputed['AcceleratorPedal'].fillna(value=diagnostics_imputed['AcceleratorPedal'].median())\n",
    "# diagnostics_imputed['CruiseControlSetSpeed'] = diagnostics_imputed['CruiseControlSetSpeed'].fillna(value=diagnostics_imputed['CruiseControlSetSpeed'].median())\n",
    "# diagnostics_imputed['EngineTimeLtd'] = diagnostics_imputed['EngineTimeLtd'].fillna(value=diagnostics_imputed['EngineTimeLtd'].median())\n",
    "# diagnostics_imputed['FuelLevel'] = diagnostics_imputed['FuelLevel'].fillna(value=diagnostics_imputed['FuelLevel'].median())\n",
    "# diagnostics_imputed['FuelTemperature'] = diagnostics_imputed['FuelTemperature'].fillna(value=diagnostics_imputed['FuelTemperature'].median())\n",
    "# diagnostics_imputed['SwitchedBatteryVoltage'] = diagnostics_imputed['SwitchedBatteryVoltage'].fillna(value=diagnostics_imputed['SwitchedBatteryVoltage'].median())\n",
    "# diagnostics_imputed['Throttle'] = diagnostics_imputed['Throttle'].fillna(value=diagnostics_imputed['Throttle'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this took 30 min and didn't stop ...\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer, KNNImputer\n",
    "# scaler = StandardScaler().fit(diagnostics_imputed)\n",
    "\n",
    "# knn_filled = scaler.inverse_transform(KNNImputer().fit_transform(scaler.transform(diagnostics_imputed)))\n",
    "\n",
    "# diagnostics_imputed = IterativeImputer().fit_transform(diagnostics_imputed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: during one of the trainings there was a particular spn-fmi code that got to the top, with importance of 0.8! It was the 46262 code and after inspecting, despite appearing only once in the dataset, since there are many events happening in a smlal timeframe around it, it got picked up as important!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Train-Test split\n",
    "\n",
    "Initially I used just a regular train-test split on the variables. However, there are trucks whose events end up mixed between both train and test split. Instead, we want to make sure that each individual truck only appears in one.\n",
    "\n",
    "I also refined the process that I initially used and combined it into a function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "print(faults['EquipmentID'].nunique())\n",
    "print(faults.loc[faults['spn'] == 5246]['EquipmentID'].nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, get the two lists of trucks that had (or not) a full derate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trucks = faults['EquipmentID'].unique()\n",
    "derate_trucks = faults.loc[faults['spn'] == 5246]['EquipmentID'].unique()\n",
    "no_derate_trucks = all_trucks[np.isin(all_trucks, derate_trucks, invert=True)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, put those lists together, marking if a derate occured (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks_df = pd.concat([\n",
    "            pd.DataFrame({'EquipmentID': derate_trucks, 'derate': 1}),\n",
    "            pd.DataFrame({'EquipmentID': no_derate_trucks, 'derate': 0}) \n",
    "            ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, use the train_test_split, by accounting for the proportion of 'derates' in both (using stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks_train, trucks_test = train_test_split(trucks_df, stratify=trucks_df['derate'], train_size = 0.8, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was just to verify that the proportions of trucks with and without derate in two samples are equal\n",
    "# print(trucks_train['derate'].value_counts(normalize=True))\n",
    "# print(trucks_test['derate'].value_counts(normalize=True))\n",
    "\n",
    "# print(faults.loc[faults['EquipmentID'].isin(trucks_train['EquipmentID'])].shape[0])\n",
    "# print(faults.loc[faults['EquipmentID'].isin(trucks_test['EquipmentID'])].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use that information to split the diagnostics and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to extract this because the train dataset only has RecordID\n",
    "records_train = faults.loc[faults['EquipmentID'].isin(trucks_train['EquipmentID'])]['RecordID']\n",
    "records_test = faults.loc[faults['EquipmentID'].isin(trucks_test['EquipmentID'])]['RecordID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_derate.loc[y_derate['RecordID'].isin(records_train)].sort_values('RecordID').drop(columns='RecordID')['target']\n",
    "y_test = y_derate.loc[y_derate['RecordID'].isin(records_test)].sort_values('RecordID').drop(columns='RecordID')['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the y_train and y_test are sorted, time to do the same for the X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_diagnostics = faults.merge(diagnostics_imputed, left_on='RecordID', right_on='FaultId', how='inner').drop(columns='FaultId')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it depends on how these get prepared, so I'll build a function. It takes the faults + diagnostic con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowize_predictors(fulldetail_faults, time_window='1d', faults_agg='max', windowize_diagnostics = True, diagnostics_agg='mean'):\n",
    "\n",
    "    # pull out the diagnostics table columns for later\n",
    "    diagnostics_cols = [col for col in fulldetail_faults.columns if col not in ['RecordID', 'spn', 'fmi', 'EquipmentID']]\n",
    "\n",
    "    # create a combined spn_fmi column to make dummies out of\n",
    "    fulldetail_faults['spn_fmi'] = ['_'.join(i) for i in zip(fulldetail_faults['spn'].astype(str), fulldetail_faults['fmi'].astype(str))]\n",
    "\n",
    "    # make dummies (one hot encode)\n",
    "    fulldetail_faults = pd.get_dummies(fulldetail_faults, columns=['spn_fmi'], prefix='spn_fmi')\n",
    "\n",
    "    # make sure the dataframe is in the right order to be able to later re-assign RecordID to it\n",
    "    fulldetail_faults = fulldetail_faults.sort_values(by=['EquipmentID', 'EventTimeStamp'])\n",
    "\n",
    "    # pull out all the Faults table columns (now one hot encoded)\n",
    "    faults_cols = ['EventTimeStamp'] + [col for col in fulldetail_faults.columns if 'spn_fmi' in col] \n",
    "\n",
    "    # rolling window function over faults - by default just taking IF a code appears in a 24 hr past window\n",
    "    faults_rolling = (\n",
    "        fulldetail_faults\n",
    "            .groupby('EquipmentID')[faults_cols]\n",
    "            .rolling(window = time_window, on = \"EventTimeStamp\")\n",
    "            .agg(faults_agg)\n",
    "            .reset_index()\n",
    "    )\n",
    "    \n",
    "    # by default I also decided to apply the same rolling window for the diagnostics part\n",
    "    # (can be turned off by setting = False, it is quick to execute)\n",
    "    if windowize_diagnostics:\n",
    "\n",
    "        # rolling window over diagnostics, by default using mean\n",
    "        diagnostics_rolling = (\n",
    "            fulldetail_faults\n",
    "                .groupby('EquipmentID')[diagnostics_cols]\n",
    "                .rolling(window = time_window, on = \"EventTimeStamp\")\n",
    "                .agg(diagnostics_agg)\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "        # joining back the faults rw to the original dataframe to get the \"RecordID\" out\n",
    "        faults_rolling = pd.merge(fulldetail_faults[['RecordID', 'spn']],\n",
    "                            faults_rolling,\n",
    "                            left_index= True,\n",
    "                            right_on = 'level_1').drop(columns='level_1')\n",
    "        \n",
    "        ###### ONLY uncomment this next line IF the derate rows are not tagged\n",
    "        # faults_rolling = faults_rolling.loc[faults_rolling['spn'] != 5246]\n",
    "\n",
    "        # joining back the diagnostics rw to the original dataframe to get the \"RecordID\" out\n",
    "        diagnostics_rolling = pd.merge(fulldetail_faults[['RecordID', 'spn']],\n",
    "                                diagnostics_rolling,\n",
    "                                left_index= True,\n",
    "                                right_on = 'level_1').drop(columns='level_1')\n",
    "        \n",
    "        ####### ONLY uncomment this next line IF the derate rows are not tagged\n",
    "        # diagnostics_rolling = diagnostics_rolling.loc[diagnostics_rolling['spn'] != 5246]\n",
    "        \n",
    "        # joining the two rolling windows\n",
    "        faults_diagnostics_rolling =  pd.merge(\n",
    "            diagnostics_rolling.drop(columns=['EquipmentID', 'EventTimeStamp', 'spn']),\n",
    "            faults_rolling.drop(columns=['EquipmentID', 'EventTimeStamp', 'spn']),\n",
    "            on = 'RecordID'\n",
    "        )\n",
    "\n",
    "    # this gets used if we only want to take into account the current diagnostics\n",
    "    # (essentially, NO rolling window for diagnostics)\n",
    "    else :\n",
    "\n",
    "        # simply get back 'RecordID' and other diagnostic columns\n",
    "        faults_diagnostics_rolling = pd.merge(\n",
    "            fulldetail_faults[['RecordID', 'spn'] + diagnostics_cols].drop(columns=['EventTimeStamp']),\n",
    "            faults_rolling.drop(columns=['EquipmentID', 'EventTimeStamp']),\n",
    "            left_index= True,\n",
    "            right_on = 'level_1').drop(columns='level_1')\n",
    "        \n",
    "        ####### ONLY uncomment this next line IF the derate rows are not tagged\n",
    "        # faults_diagnostics_rolling = faults_diagnostics_rolling.loc[faults_diagnostics_rolling['spn'] != 5246]\n",
    "        \n",
    "        faults_diagnostics_rolling = faults_diagnostics_rolling.drop(columns='spn')\n",
    "        \n",
    "    predictor_train = (\n",
    "        faults_diagnostics_rolling\n",
    "        .loc[faults_diagnostics_rolling['RecordID']\n",
    "             .isin(records_train)]\n",
    "        .sort_values('RecordID')\n",
    "        .drop(columns='RecordID')\n",
    "    )\n",
    "    predictor_test = (\n",
    "        faults_diagnostics_rolling\n",
    "        .loc[faults_diagnostics_rolling['RecordID']\n",
    "             .isin(records_test)]\n",
    "        .sort_values('RecordID')\n",
    "        .drop(columns='RecordID')\n",
    "    )\n",
    "\n",
    "    return predictor_train, predictor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = windowize_predictors(faults_diagnostics, time_window='7d', faults_agg='max', windowize_diagnostics=True, diagnostics_agg='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = Pipeline(\n",
    "    steps = [\n",
    "        ('gb', GradientBoostingClassifier(verbose=True)) #, n_estimators =350, learning_rate=0.03\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('confusion matrix')\n",
    "# print(confusion_matrix(y_train, gbr.predict(X_train)))\n",
    "# print('\\n')\n",
    "# print('classification report')\n",
    "# print(classification_report(y_train, gbr.predict(X_train)))\n",
    "# print('\\n')\n",
    "\n",
    "# importances = pd.DataFrame({\n",
    "#     'variable': gbr.feature_names_in_,\n",
    "#     'importance': gbr['gb'].feature_importances_\n",
    "# })\n",
    "\n",
    "# print('Variable Importances:')\n",
    "# display(importances.sort_values('importance', ascending = False).head(20))\n",
    "\n",
    "# print('------ TEST')\n",
    "# print(confusion_matrix(y_test, gbr.predict(X_test)))\n",
    "# print('ROC AUC Score')\n",
    "# print(roc_auc_score(y_true=y_test, y_score=gbr.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = SMOTE(k_neighbors=5, random_state=42)\n",
    "\n",
    "X_smote, y_smote = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2303           19.92m\n",
      "         2           1.1026           19.59m\n",
      "         3           0.9966           19.41m\n",
      "         4           0.9074           19.01m\n",
      "         5           0.8317           18.65m\n",
      "         6           0.7720           18.47m\n",
      "         7           0.7148           18.57m\n",
      "         8           0.6696           18.26m\n",
      "         9           0.6246           17.92m\n",
      "        10           0.5903           17.69m\n",
      "        20           0.3474           16.68m\n",
      "        30           0.2600           14.78m\n",
      "        40           0.2188           12.79m\n",
      "        50           0.1963           10.70m\n",
      "        60           0.1805            8.60m\n",
      "        70           0.1681            6.46m\n",
      "        80           0.1589            4.32m\n",
      "        90           0.1501            2.16m\n",
      "       100           0.1421            0.00s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gb&#x27;, GradientBoostingClassifier(verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gb&#x27;, GradientBoostingClassifier(verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(verbose=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gb', GradientBoostingClassifier(verbose=True))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is going to re-fit from scratch, unless we set warm_start=True\n",
    "# also, simply add this line to all X_ variables if you want to exclude 5246 influencing the model:\n",
    "# .drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col])\n",
    "# for partial derates, this one is much worse\n",
    "# .drop(columns=[col for col in X_smote.columns if 'spn_fmi_1569' in col])\n",
    "gbr.fit(X_smote.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]), y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[428651  13269]\n",
      " [    21    860]]\n",
      "\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98    441920\n",
      "           1       0.06      0.98      0.11       881\n",
      "\n",
      "    accuracy                           0.97    442801\n",
      "   macro avg       0.53      0.97      0.55    442801\n",
      "weighted avg       1.00      0.97      0.98    442801\n",
      "\n",
      "\n",
      "\n",
      "Variable Importances:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LampStatus</td>\n",
       "      <td>0.397541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FuelTemperature</td>\n",
       "      <td>0.315414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>spn_fmi_1569_31</td>\n",
       "      <td>0.116364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>activeTransitionCount</td>\n",
       "      <td>0.048253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CruiseControlSetSpeed</td>\n",
       "      <td>0.020625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>spn_fmi_111_17</td>\n",
       "      <td>0.019034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>spn_fmi_3362_31</td>\n",
       "      <td>0.009580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>spn_fmi_5394_5</td>\n",
       "      <td>0.009374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>spn_fmi_1787_11</td>\n",
       "      <td>0.005330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>spn_fmi_3363_3</td>\n",
       "      <td>0.004143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>spn_fmi_1209_2</td>\n",
       "      <td>0.003826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>spn_fmi_596_31</td>\n",
       "      <td>0.002782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Throttle</td>\n",
       "      <td>0.002678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SwitchedBatteryVoltage</td>\n",
       "      <td>0.002656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DistanceLtd</td>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EngineRpm</td>\n",
       "      <td>0.002395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>spn_fmi_524287_31</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>spn_fmi_3251_2</td>\n",
       "      <td>0.002218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>spn_fmi_4094_18</td>\n",
       "      <td>0.002203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>spn_fmi_157_18</td>\n",
       "      <td>0.001915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   variable  importance\n",
       "15               LampStatus    0.397541\n",
       "13          FuelTemperature    0.315414\n",
       "163         spn_fmi_1569_31    0.116364\n",
       "20    activeTransitionCount    0.048253\n",
       "2     CruiseControlSetSpeed    0.020625\n",
       "90           spn_fmi_111_17    0.019034\n",
       "389         spn_fmi_3362_31    0.009580\n",
       "636          spn_fmi_5394_5    0.009374\n",
       "219         spn_fmi_1787_11    0.005330\n",
       "392          spn_fmi_3363_3    0.004143\n",
       "109          spn_fmi_1209_2    0.003826\n",
       "731          spn_fmi_596_31    0.002782\n",
       "18                 Throttle    0.002678\n",
       "17   SwitchedBatteryVoltage    0.002656\n",
       "3               DistanceLtd    0.002519\n",
       "8                 EngineRpm    0.002395\n",
       "622       spn_fmi_524287_31    0.002351\n",
       "371          spn_fmi_3251_2    0.002218\n",
       "467         spn_fmi_4094_18    0.002203\n",
       "167          spn_fmi_157_18    0.001915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TEST\n",
      "confusion matrix\n",
      "[[100231   3439]\n",
      " [    28    175]]\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98    103670\n",
      "           1       0.05      0.86      0.09       203\n",
      "\n",
      "    accuracy                           0.97    103873\n",
      "   macro avg       0.52      0.91      0.54    103873\n",
      "weighted avg       1.00      0.97      0.98    103873\n",
      "\n",
      "ROC AUC Score\n",
      "0.9884836595468474\n"
     ]
    }
   ],
   "source": [
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_train, gbr.predict(X_train.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))))\n",
    "print('\\n')\n",
    "print('classification report')\n",
    "print(classification_report(y_train, gbr.predict(X_train.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))))\n",
    "print('\\n')\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'variable': gbr.feature_names_in_,\n",
    "    'importance': gbr['gb'].feature_importances_\n",
    "})\n",
    "\n",
    "print('Variable Importances:')\n",
    "display(importances.sort_values('importance', ascending = False).head(20))\n",
    "\n",
    "print('------ TEST')\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_test, gbr.predict(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))))\n",
    "print('classification report')\n",
    "print(classification_report(y_test, gbr.predict(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))))\n",
    "print('ROC AUC Score')\n",
    "print(roc_auc_score(y_true=y_test, y_score=gbr.predict_proba(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/gbr_model_28.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load model\n",
    "# gbr = load('../models/gbr_model_1.joblib') \n",
    "\n",
    "# to save model\n",
    "#dump(gbr, '../models/gbr_model_28.joblib') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides saving the models, I will construct a json file that describes how they were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dump = {\n",
    "    'file_path' : '../models/gbr_model_28.joblib',\n",
    "    'targets' : 'any row where a derate (5246) happens in the next 3 hours',\n",
    "    'diagnostics_file' : 'used imputer to average data per truck and then simple mean to average any remaining nulls',\n",
    "    'train_test_split' : 'using trucks and assuring same ratio of derate and nonderate',\n",
    "    'windowize_predictors': {'dataframe': 'merged faults and diagnostics',\n",
    "                             'how far in the past to aggregate' : '7 days',\n",
    "                             'how to aggregate the one-hot encoded spn_fmi': 'max (default)',\n",
    "                             'use rolling window on diagnostics?' : 'True ',\n",
    "                             'how to aggregate diagnostics data' : 'max'},\n",
    "    'pipeline' : {'step 1': 'GradientBoostingClassifier (default values)'},\n",
    "    'rebalancing' : {'over or under fitting': 'used SMOTE(k_neighbors=5, random_state=42)',\n",
    "                     'variables used': 'eliminated 5246 columns (derates), by using the .drop on X_train and X_test'}\n",
    "\n",
    "}\n",
    "\n",
    "tmp_matrix = confusion_matrix(y_train, gbr.predict(X_train.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col])))\n",
    "\n",
    "to_dump['train_confusion_matrix'] = {'TN': int(tmp_matrix[0][0]),\n",
    "                                     'FP': int(tmp_matrix[0][1]),\n",
    "                                     'FN': int(tmp_matrix[1][0]),\n",
    "                                     'TP': int(tmp_matrix[1][1])}\n",
    "\n",
    "tmp_matrix = confusion_matrix(y_test, gbr.predict(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col])))\n",
    "\n",
    "to_dump['test_confusion_matrix'] = {'TN': int(tmp_matrix[0][0]),\n",
    "                                    'FP': int(tmp_matrix[0][1]),\n",
    "                                    'FN': int(tmp_matrix[1][0]),\n",
    "                                    'TP': int(tmp_matrix[1][1])}\n",
    "\n",
    "\n",
    "to_dump['test_rocaouc_score'] = roc_auc_score(y_true=y_test, y_score=gbr.predict_proba(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col]))[:,1])\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'variable': gbr.feature_names_in_,\n",
    "    'importance': gbr['gb'].feature_importances_\n",
    "})\n",
    "\n",
    "importances = importances.sort_values('importance', ascending = False).head(20)\n",
    "\n",
    "tmp_dict={}\n",
    "\n",
    "for index, row in importances.iterrows():\n",
    "    tmp_dict[row[\"variable\"]] = row['importance']\n",
    "\n",
    "to_dump['top20_fature_importances'] = tmp_dict\n",
    "\n",
    "\n",
    "json_object = json.dumps(to_dump, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../models/gbr_model_28.json', 'w') as outfile:\n",
    "#     outfile.write(json_object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into the most promising model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First few steps are straightforward as outlined below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Michael's suggestion was to check if any of the false positives actually happen to have a derate within 24 hours. Adding that to the dataframe from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that I want to use to look at the predictions\n",
    "gbr_best = load('../models/gbr_model_28.joblib')\n",
    "\n",
    "# get the target values \n",
    "# note: the model might have been trained on a different time window, but if it correctly predicts derates further down the road, that is perfectly fine\n",
    "# that is why, using Michael's suggestion, always compare models to the 24 hr derate window\n",
    "y_derate = pd.read_pickle('../data/target_derate24h.pkl')\n",
    "\n",
    "y_comparison = y_derate.loc[y_derate['RecordID'].isin(records_test)].sort_values('RecordID')\n",
    "\n",
    "# preditc y values based on model\n",
    "y_pred = gbr_best.predict(X_test.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col])) #.drop(columns=[col for col in X_smote.columns if 'spn_fmi_5246' in col])\n",
    "\n",
    "# put all of it together in a dataframe\n",
    "y_comparison['predicted'] = y_pred\n",
    "\n",
    "# merge it back to get the complete faults info\n",
    "test_results = pd.merge(faults, y_comparison, on='RecordID', how='inner')\n",
    "\n",
    "# flag the rows where the derate occurred\n",
    "test_results['dummy_derate'] = np.where(test_results['spn'] == 5246, 1, 0)\n",
    "\n",
    "# sort test_results in the right order since that's what's needed for rolling windows\n",
    "# note that the dummy_derate now needs to be last in case of a tie (as opposed to when we were looking in the future)\n",
    "test_results = test_results.sort_values(by=['EquipmentID','EventTimeStamp','dummy_derate'], ascending=[False, True, True])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> IMPORTANT note: initially I did not consider the fact that there will be some codes following a derate event whose predictions should be ignored (likely truck being worked on etc).\n",
    "\n",
    "There's 208 rows in the test dataset that follow within 1 day after a derate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_derate = (\n",
    "    test_results\n",
    "        .groupby('EquipmentID')[['EventTimeStamp', 'dummy_derate']]\n",
    "        .rolling(window = '7d', on = \"EventTimeStamp\")\n",
    "        .sum()\n",
    "        .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.merge(test_results.drop(columns=['dummy_derate']),\n",
    "        after_derate.drop(columns=['EquipmentID', 'EventTimeStamp']),\n",
    "        left_index= True,\n",
    "        right_on = 'level_1').drop(columns='level_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_results.loc[(test_results['dummy_derate'] == 0.) | (test_results['spn'] == 5246)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the confusion Matrix for the model 5:\n",
    "- \"TN\": 100535\n",
    "- \"FP\": 3099\n",
    "- \"FN\": 37\n",
    "- \"TP\": 202\n",
    "\n",
    "This is the confusion Matrix for the model 13:\n",
    "- \"TN\": 99699\n",
    "- \"FP\": 3726\n",
    "- \"FN\": 81\n",
    "- \"TP\": 367\n",
    "\n",
    "This is the confusion Matrix for the model 15:\n",
    "- \"TN\": 99585\n",
    "- \"FP\": 3840\n",
    "- \"FN\": 78\n",
    "- \"TP\": 370\n",
    "\n",
    "This is the confusion Matrix for the model 26:\n",
    "- \"TN\": 100410\n",
    "- \"FP\": 3224\n",
    "- \"FN\": 30\n",
    "- \"TP\": 209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the false positives\n",
    "false_positive = test_results.loc[(test_results['target'] == 0) & (test_results['predicted'] == 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic here is that if I use the rolling window again, I can sum up on the \"predicted\" values. Any sums that are more than 1 indicate repeated values. I.e. they show that those predictions occur within 24 hours and therefore, they were not actually separate predictions of the model.\n",
    "\n",
    "On top of that, I realized that some \"predicted\" derates were happening AFTER a derate occured. So within the next hour or two. Those also shouldn't be counted as false predictions since the truck is likely being worked on.\n",
    "\n",
    "In order to get the unique false predictions, we count how many times 'predicted' was 1.\n",
    "\n",
    "**results**:\n",
    "- model 5: 819 out of 3099 are false positives\n",
    "- model 13: 644 out of 3726 are false positives\n",
    "- model 15: 609 out of 2840 are false positives\n",
    "- model 26: 496 out of 3224 are false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first off, need to see how many of these false positives occur in the next day or so after the derate\n",
    "# because the truck will likely be repaired and / on worked on, so the conditions for the derate might still be there to be picked up on by the model\n",
    "\n",
    "false_positive = (\n",
    "    false_positive\n",
    "    .groupby('EquipmentID')[['EventTimeStamp', 'predicted']]\n",
    "    .rolling(window = '1d', on = \"EventTimeStamp\")\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>EventTimeStamp</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EquipmentID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <th>653</th>\n",
       "      <td>2015-04-05 22:00:30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1366</th>\n",
       "      <th>1140</th>\n",
       "      <td>2015-06-10 01:45:22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>2015-07-01 12:38:58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>2015-10-26 14:33:56</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>2015-10-28 15:14:30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <th>98086</th>\n",
       "      <td>2020-02-12 00:06:58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <th>98117</th>\n",
       "      <td>2017-06-06 09:59:45</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <th>98431</th>\n",
       "      <td>2017-04-04 09:46:14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">309</th>\n",
       "      <th>103417</th>\n",
       "      <td>2018-03-12 12:33:31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103420</th>\n",
       "      <td>2018-03-16 05:43:56</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        EventTimeStamp  predicted\n",
       "EquipmentID                                      \n",
       "1350        653    2015-04-05 22:00:30        1.0\n",
       "1366        1140   2015-06-10 01:45:22        1.0\n",
       "            1175   2015-07-01 12:38:58        1.0\n",
       "            1302   2015-10-26 14:33:56        1.0\n",
       "            1303   2015-10-28 15:14:30        1.0\n",
       "...                                ...        ...\n",
       "2015        98086  2020-02-12 00:06:58        1.0\n",
       "2019        98117  2017-06-06 09:59:45        1.0\n",
       "2045        98431  2017-04-04 09:46:14        1.0\n",
       "309         103417 2018-03-12 12:33:31        1.0\n",
       "            103420 2018-03-16 05:43:56        1.0\n",
       "\n",
       "[534 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive.loc[false_positive['predicted'] == 1.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar approach to get the false negatives, except now we invert target and predicted.\n",
    "\n",
    "**results**:\n",
    "- model 5: 11 out of 37 are false negatives\n",
    "- model 13: 24 out of 81 are false negatives\n",
    "- model 15: 24 out of 78 are false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the false negatives\n",
    "false_negative = test_results.loc[(test_results['target'] == 1) & (test_results['predicted'] == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negative = (\n",
    "    false_negative\n",
    "    .groupby('EquipmentID')[['EventTimeStamp', 'target']]\n",
    "    .rolling(window = '1d', on = \"EventTimeStamp\")\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "len(false_negative.loc[false_negative['target'] == 1.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, looking at the true positives\n",
    "\n",
    "**results**: \n",
    "- 67 out of 202 are true positives, out of which 21 are predicted at least 2 hours in advance\n",
    "- 68 out of 367 are true positives, out of which 41 are predicted at least 2 hours in advance\n",
    "- 68 out of 367 are true positives, out of which 43 are predicted at least 2 hours in advance\n",
    "- 68 out of 209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the true positive\n",
    "true_positive = test_results.loc[(test_results['target'] == 1) & (test_results['predicted'] == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = (\n",
    "    true_positive\n",
    "    .groupby('EquipmentID')[['EventTimeStamp', 'RecordID', 'predicted', 'target']]\n",
    "    .rolling(window = '1d', on = \"EventTimeStamp\")\n",
    "    .agg({'RecordID': lambda x: x[-1], 'predicted': 'sum', 'target': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "true_positive['RecordID'] = true_positive['RecordID'].astype(int)\n",
    "\n",
    "true_positive = true_positive.loc[true_positive['predicted'] == 1.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: do not use iterrows to modify the dataframe it's being iterated over!! the results are not guaranteed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EquipmentID</th>\n",
       "      <th>EventTimeStamp</th>\n",
       "      <th>RecordID</th>\n",
       "      <th>predicted</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1329</td>\n",
       "      <td>2015-02-25 13:53:08</td>\n",
       "      <td>5715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1339</td>\n",
       "      <td>2015-06-12 08:24:15</td>\n",
       "      <td>85259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1366</td>\n",
       "      <td>2015-06-11 10:08:58</td>\n",
       "      <td>84237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1366</td>\n",
       "      <td>2015-07-03 15:10:45</td>\n",
       "      <td>109732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1366</td>\n",
       "      <td>2015-09-23 07:25:22</td>\n",
       "      <td>214277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1922</td>\n",
       "      <td>2019-07-07 11:13:03</td>\n",
       "      <td>1176722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1928</td>\n",
       "      <td>2018-08-03 12:34:33</td>\n",
       "      <td>1042659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1970</td>\n",
       "      <td>2019-04-28 17:50:36</td>\n",
       "      <td>1153464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2004</td>\n",
       "      <td>2019-07-03 07:08:25</td>\n",
       "      <td>1176070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2009</td>\n",
       "      <td>2017-06-24 10:42:36</td>\n",
       "      <td>815232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    EquipmentID      EventTimeStamp  RecordID  predicted  target\n",
       "0          1329 2015-02-25 13:53:08      5715        1.0     1.0\n",
       "2          1339 2015-06-12 08:24:15     85259        1.0     1.0\n",
       "4          1366 2015-06-11 10:08:58     84237        1.0     1.0\n",
       "14         1366 2015-07-03 15:10:45    109732        1.0     1.0\n",
       "16         1366 2015-09-23 07:25:22    214277        1.0     1.0\n",
       "..          ...                 ...       ...        ...     ...\n",
       "254        1922 2019-07-07 11:13:03   1176722        1.0     1.0\n",
       "265        1928 2018-08-03 12:34:33   1042659        1.0     1.0\n",
       "266        1970 2019-04-28 17:50:36   1153464        1.0     1.0\n",
       "273        2004 2019-07-03 07:08:25   1176070        1.0     1.0\n",
       "275        2009 2017-06-24 10:42:36    815232        1.0     1.0\n",
       "\n",
       "[68 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "derate_times = []\n",
    "\n",
    "# find the timestamp of next actual derate that happens\n",
    "for index, row in true_positive.iterrows():\n",
    "    derate_times.append(\n",
    "        faults.loc[(faults['EquipmentID'] == str(row['EquipmentID']))\n",
    "                   & (faults['spn'] == 5246) \n",
    "                   & (faults['EventTimeStamp'] >= row['EventTimeStamp'])]\n",
    "                   .iloc[0]['EventTimeStamp']\n",
    "    )\n",
    "\n",
    "# save that in the dataframe\n",
    "true_positive['derateTimeStamp'] = derate_times\n",
    "\n",
    "# measure how soon the prediction happened before the derate\n",
    "true_positive['timediff'] = true_positive['derateTimeStamp'] - true_positive['EventTimeStamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_positive.loc[true_positive['timediff'] > timedelta(hours= 2)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results.loc[(test_results['target'] == 0) & (test_results['predicted'] == 1)]['activeTransitionCount'].mean()\n",
    "# test_results.loc[test_results['target']]['activeTransitionCount'].mean()\n",
    "# faults['activeTransitionCount'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
